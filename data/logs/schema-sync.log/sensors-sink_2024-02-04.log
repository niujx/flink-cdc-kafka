14:36:17.866 [background-preinit] INFO  org.hibernate.validator.internal.util.Version - HV000001: Hibernate Validator 6.2.0.Final
14:36:18.144 [main] INFO  com.rock.cdc.kafka.schema.sync.App - Starting App using Java 1.8.0_281 on bogon with PID 6553 (/Users/yanshi/work/github/flink-cdc-kafka/flink-cdc-pipeline-connector-kafka-schema-sync/target/classes started by yanshi in /Users/yanshi/work/github/flink-cdc-kafka)
14:36:18.164 [main] INFO  com.rock.cdc.kafka.schema.sync.App - The following profiles are active: production
14:36:19.484 [main] WARN  org.springframework.boot.context.config.ConfigDataEnvironment - Property 'spring.profiles' imported from location 'class path resource [application.yaml]' is invalid and should be replaced with 'spring.config.activate.on-profile' [origin: class path resource [application.yaml] - 45:13]
14:36:19.486 [main] WARN  org.springframework.boot.context.config.ConfigDataEnvironment - Property 'spring.profiles' imported from location 'class path resource [application.yaml]' is invalid and should be replaced with 'spring.config.activate.on-profile' [origin: class path resource [application.yaml] - 10:13]
14:36:24.319 [main] INFO  org.springframework.integration.config.DefaultConfiguringBeanFactoryPostProcessor - No bean named 'errorChannel' has been explicitly defined. Therefore, a default PublishSubscribeChannel will be created.
14:36:24.355 [main] INFO  org.springframework.integration.config.DefaultConfiguringBeanFactoryPostProcessor - No bean named 'integrationHeaderChannelRegistry' has been explicitly defined. Therefore, a default DefaultHeaderChannelRegistry will be created.
14:36:24.781 [main] INFO  org.springframework.context.support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker - Bean 'org.springframework.cloud.stream.config.BindersHealthIndicatorAutoConfiguration' of type [org.springframework.cloud.stream.config.BindersHealthIndicatorAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
14:36:24.790 [main] INFO  org.springframework.context.support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker - Bean 'bindersHealthContributor' of type [org.springframework.cloud.stream.config.BindersHealthIndicatorAutoConfiguration$BindersHealthContributor] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
14:36:24.795 [main] INFO  org.springframework.context.support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker - Bean 'bindersHealthIndicatorListener' of type [org.springframework.cloud.stream.config.BindersHealthIndicatorAutoConfiguration$BindersHealthIndicatorListener] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
14:36:24.848 [main] INFO  org.springframework.context.support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker - Bean 'org.springframework.integration.config.IntegrationManagementConfiguration' of type [org.springframework.integration.config.IntegrationManagementConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
14:36:24.929 [main] INFO  org.springframework.context.support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker - Bean 'integrationChannelResolver' of type [org.springframework.integration.support.channel.BeanFactoryChannelResolver] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
14:36:24.932 [main] INFO  org.springframework.context.support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker - Bean 'integrationDisposableAutoCreatedBeans' of type [org.springframework.integration.config.annotation.Disposables] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
14:36:26.439 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port(s): 9999 (http)
14:36:26.473 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-9999"]
14:36:26.503 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
14:36:26.505 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/9.0.52]
14:36:27.094 [main] INFO  org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
14:36:27.094 [main] INFO  org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 7601 ms
14:36:44.800 [main] INFO  org.springframework.cloud.stream.messaging.DirectWithAttributesChannel - Channel 'application.sinkProcess-in-0' has 1 subscriber(s).
14:36:46.004 [main] INFO  org.springframework.boot.actuate.endpoint.web.EndpointLinksResolver - Exposing 1 endpoint(s) beneath base path '/actuator'
14:36:46.188 [main] INFO  org.springframework.integration.endpoint.EventDrivenConsumer - Adding {logging-channel-adapter:_org.springframework.integration.errorLogger} as a subscriber to the 'errorChannel' channel
14:36:46.189 [main] INFO  org.springframework.integration.channel.PublishSubscribeChannel - Channel 'application.errorChannel' has 1 subscriber(s).
14:36:46.189 [main] INFO  org.springframework.integration.endpoint.EventDrivenConsumer - started bean '_org.springframework.integration.errorLogger'
14:36:46.190 [main] INFO  org.springframework.integration.endpoint.EventDrivenConsumer - Adding {service-activator:app.error.serviceActivator} as a subscriber to the 'sensors-event-log.sensors-group-20210825.errors' channel
14:36:46.190 [main] INFO  org.springframework.integration.channel.DirectChannel - Channel 'application.sensors-event-log.sensors-group-20210825.errors' has 1 subscriber(s).
14:36:46.190 [main] INFO  org.springframework.integration.endpoint.EventDrivenConsumer - started bean 'app.error.serviceActivator'
14:36:46.192 [main] INFO  org.springframework.cloud.stream.binder.DefaultBinderFactory - Creating binder: kafka
14:36:46.866 [main] INFO  org.springframework.cloud.stream.binder.DefaultBinderFactory - Caching the binder: kafka
14:36:46.867 [main] INFO  org.springframework.cloud.stream.binder.DefaultBinderFactory - Retrieving cached binder: kafka
14:36:47.028 [main] INFO  org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
	bootstrap.servers = [10.195.40.142:9092, 10.195.40.117:9092, 10.195.40.119:9092, 10.195.40.68:9092, 10.195.40.80:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

14:36:47.280 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.7.1
14:36:47.280 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 61dbce85d0d41457
14:36:47.280 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1707028607259
14:36:47.283 [main] INFO  org.springframework.cloud.stream.binder.kafka.provisioning.KafkaTopicProvisioner - Auto creation of topics is disabled.
14:36:48.272 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.common.utils.AppInfoParser - App info kafka.admin.client for adminclient-1 unregistered
14:36:48.273 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.clients.admin.internals.AdminMetadataManager - [AdminClient clientId=adminclient-1] Metadata update failed
org.apache.kafka.common.errors.TimeoutException: Call(callName=fetchMetadata, deadlineMs=1707028637283, tries=1, nextAllowedTryMs=-9223372036854775709) timed out at 9223372036854775807 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: fetchMetadata
14:36:48.297 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
14:36:48.297 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
14:36:48.298 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
14:36:48.336 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.195.40.142:9092, 10.195.40.117:9092, 10.195.40.119:9092, 10.195.40.68:9092, 10.195.40.80:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-yanshiaaaaa-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = yanshiaaaaa
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

14:36:48.466 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.7.1
14:36:48.466 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 61dbce85d0d41457
14:36:48.467 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1707028608466
14:36:48.858 [main] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-yanshiaaaaa-1, groupId=yanshiaaaaa] Cluster ID: gI_tQQPqQQ-sWskEhiiviQ
14:36:48.868 [main] INFO  org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
14:36:48.869 [main] INFO  org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
14:36:48.869 [main] INFO  org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
14:36:48.872 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-yanshiaaaaa-1 unregistered
14:36:48.958 [main] INFO  org.springframework.cloud.stream.binder.BinderErrorChannel - Channel 'catalog-meta-data.yanshiaaaaa.errors' has 1 subscriber(s).
14:36:48.958 [main] INFO  org.springframework.cloud.stream.binder.BinderErrorChannel - Channel 'catalog-meta-data.yanshiaaaaa.errors' has 0 subscriber(s).
14:36:48.959 [main] INFO  org.springframework.cloud.stream.binder.BinderErrorChannel - Channel 'catalog-meta-data.yanshiaaaaa.errors' has 1 subscriber(s).
14:36:48.959 [main] INFO  org.springframework.cloud.stream.binder.BinderErrorChannel - Channel 'catalog-meta-data.yanshiaaaaa.errors' has 2 subscriber(s).
14:36:49.036 [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 100
	auto.offset.reset = earliest
	bootstrap.servers = [10.195.40.142:9092, 10.195.40.117:9092, 10.195.40.119:9092, 10.195.40.68:9092, 10.195.40.80:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-yanshiaaaaa-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = yanshiaaaaa
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

14:36:49.048 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.7.1
14:36:49.048 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 61dbce85d0d41457
14:36:49.049 [main] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1707028609048
14:36:49.061 [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Subscribed to topic(s): catalog-meta-data
14:36:49.111 [main] INFO  org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter - started org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter@1418a1bd
14:36:49.122 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-9999"]
14:36:49.193 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Cluster ID: gI_tQQPqQQ-sWskEhiiviQ
14:36:49.207 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Discovered group coordinator 10.195.40.80:9092 (id: 2147483642 rack: null)
14:36:49.210 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] (Re-)joining group
14:36:49.213 [main] INFO  org.springframework.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port(s): 9999 (http) with context path ''
14:36:49.341 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] (Re-)joining group
14:36:49.371 [main] INFO  com.rock.cdc.kafka.schema.sync.App - Started App in 33.656 seconds (JVM running for 51.301)
14:36:52.388 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Successfully joined group with generation Generation{generationId=1, memberId='consumer-yanshiaaaaa-2-f694f908-bccd-4551-9fcb-9509c1e3d68e', protocol='range'}
14:36:52.393 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Finished assignment for group at generation 1: {consumer-yanshiaaaaa-2-f694f908-bccd-4551-9fcb-9509c1e3d68e=Assignment(partitions=[catalog-meta-data-0, catalog-meta-data-1, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-6])}
14:36:52.441 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Successfully synced group in generation Generation{generationId=1, memberId='consumer-yanshiaaaaa-2-f694f908-bccd-4551-9fcb-9509c1e3d68e', protocol='range'}
14:36:52.444 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Notifying assignor about the new Assignment(partitions=[catalog-meta-data-0, catalog-meta-data-1, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-6])
14:36:52.452 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Adding newly assigned partitions: catalog-meta-data-6, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-0, catalog-meta-data-1
14:36:52.492 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-6
14:36:52.492 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-4
14:36:52.493 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-5
14:36:52.493 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-2
14:36:52.494 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-3
14:36:52.494 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-0
14:36:52.494 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-1
14:36:52.564 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.68:9092 (id: 4 rack: null)], epoch=absent}}.
14:36:52.567 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.119:9092 (id: 3 rack: null)], epoch=absent}}.
14:36:52.567 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.119:9092 (id: 3 rack: null)], epoch=absent}}.
14:36:52.567 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.80:9092 (id: 5 rack: null)], epoch=absent}}.
14:36:52.568 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.142:9092 (id: 1 rack: null)], epoch=absent}}.
14:36:52.573 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.117:9092 (id: 2 rack: null)], epoch=absent}}.
14:36:52.574 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.117:9092 (id: 2 rack: null)], epoch=absent}}.
14:36:52.575 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.springframework.cloud.stream.binder.kafka.KafkaMessageChannelBinder$2 - yanshiaaaaa: partitions assigned: [catalog-meta-data-6, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-0, catalog-meta-data-1]
14:37:10.434 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.conf.HiveConf - Found configuration file null
14:37:11.196 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Setting hive conf dir as /Users/yanshi
14:37:11.269 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:37:11.744 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Created HiveCatalog 'ods_test'
14:37:15.464 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Trying to connect to metastore with URI thrift://bigdata-namenode-41-101.bigdata.prod:9083
14:37:15.950 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 1
14:37:16.042 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] WARN  org.apache.hadoop.security.ShellBasedUnixGroupsMapping - unable to return groups for user gmjk
org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException: The user name 'gmjk' is not found. id: gmjk: no such user
id: gmjk: no such user

	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:387) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:321) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:270) ~[hadoop-common-3.1.3.jar:?]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3952) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958) ~[guava-27.0-jre.jar:?]
	at org.apache.hadoop.security.Groups.getGroups(Groups.java:228) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1587) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1575) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:534) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:224) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_281]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_281]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_281]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_281]
	at org.apache.hadoop.hive.metastore.utils.JavaUtils.newInstance(JavaUtils.java:84) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:95) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:148) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_281]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_281]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]
	at org.apache.flink.table.catalog.hive.client.HiveShimV310.getHiveMetastoreClient(HiveShimV310.java:140) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:288) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:89) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:79) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:32) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:303) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.lambda$getCatalog$0(CatalogManager.java:405) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at java.util.Optional.map(Optional.java:215) ~[?:1.8.0_281]
	at org.apache.flink.table.catalog.CatalogManager.getCatalog(CatalogManager.java:402) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1287) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.alterTable(CatalogManager.java:1141) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at com.rock.cdc.kafka.schema.sync.App.handAlterTable(App.java:132) ~[classes/:?]
	at com.rock.cdc.kafka.schema.sync.App.lambda$sinkProcess$0(App.java:81) ~[classes/:?]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.invokeConsumer(SimpleFunctionRegistry.java:854) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.doApply(SimpleFunctionRegistry.java:643) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.apply(SimpleFunctionRegistry.java:489) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.PartitionAwareFunctionWrapper.apply(PartitionAwareFunctionWrapper.java:77) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionWrapper.apply(FunctionConfiguration.java:727) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1.handleMessageInternal(FunctionConfiguration.java:560) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:56) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:115) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:133) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:106) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:72) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:317) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:272) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:187) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:166) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:47) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:109) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:208) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.sendMessageIfAny(KafkaMessageDrivenChannelAdapter.java:398) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:79) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:457) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:431) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.lambda$onMessage$0(RetryingMessageListenerAdapter.java:123) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:329) [spring-retry-1.3.1.jar:?]
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:255) [spring-retry-1.3.1.jar:?]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:117) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:41) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2323) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2304) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2218) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2143) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2025) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1707) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161) [spring-kafka-2.7.6.jar:2.7.6]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_281]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_281]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_281]
14:37:16.082 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.
14:37:16.082 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.metastore.HiveMetaStoreClient ugi=gmjk (auth:SIMPLE) retries=1 delay=1 lifetime=0
14:37:16.601 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Connected to Hive metastore
14:37:39.651 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Setting hive conf dir as /Users/yanshi
14:37:40.445 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Created HiveCatalog 'ods_test'
14:37:40.516 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Trying to connect to metastore with URI thrift://bigdata-hiveserver-41-16.bigdata.prod:9083
14:37:40.522 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 2
14:37:40.679 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.
14:37:40.680 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.metastore.HiveMetaStoreClient ugi=gmjk (auth:SIMPLE) retries=1 delay=1 lifetime=0
14:37:40.682 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Connected to Hive metastore
14:38:05.589 [kafka-coordinator-heartbeat-thread | yanshiaaaaa] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Group coordinator 10.195.40.80:9092 (id: 2147483642 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response.isDisconnected: false. Rediscovery will be attempted.
14:38:05.697 [kafka-coordinator-heartbeat-thread | yanshiaaaaa] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Discovered group coordinator 10.195.40.80:9092 (id: 2147483642 rack: null)
14:38:06.219 [kafka-coordinator-heartbeat-thread | yanshiaaaaa] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Attempt to heartbeat with Generation{generationId=1, memberId='consumer-yanshiaaaaa-2-f694f908-bccd-4551-9fcb-9509c1e3d68e', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
14:38:08.993 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Setting hive conf dir as /Users/yanshi
14:38:09.723 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Created HiveCatalog 'ods_test'
14:38:09.791 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Trying to connect to metastore with URI thrift://bigdata-namenode-41-101.bigdata.prod:9083
14:38:09.795 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 3
14:38:09.851 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] WARN  org.apache.hadoop.security.ShellBasedUnixGroupsMapping - unable to return groups for user gmjk
org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException: The user name 'gmjk' is not found. id: gmjk: no such user
id: gmjk: no such user

	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:387) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:321) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:270) ~[hadoop-common-3.1.3.jar:?]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3952) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958) ~[guava-27.0-jre.jar:?]
	at org.apache.hadoop.security.Groups.getGroups(Groups.java:228) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1587) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1575) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:534) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:224) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_281]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_281]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_281]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_281]
	at org.apache.hadoop.hive.metastore.utils.JavaUtils.newInstance(JavaUtils.java:84) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:95) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:148) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_281]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_281]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]
	at org.apache.flink.table.catalog.hive.client.HiveShimV310.getHiveMetastoreClient(HiveShimV310.java:140) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:288) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:89) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:79) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:32) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:303) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.lambda$getCatalog$0(CatalogManager.java:405) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at java.util.Optional.map(Optional.java:215) ~[?:1.8.0_281]
	at org.apache.flink.table.catalog.CatalogManager.getCatalog(CatalogManager.java:402) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1287) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.alterTable(CatalogManager.java:1141) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at com.rock.cdc.kafka.schema.sync.App.handAlterTable(App.java:132) ~[classes/:?]
	at com.rock.cdc.kafka.schema.sync.App.lambda$sinkProcess$0(App.java:81) ~[classes/:?]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.invokeConsumer(SimpleFunctionRegistry.java:854) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.doApply(SimpleFunctionRegistry.java:643) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.apply(SimpleFunctionRegistry.java:489) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.PartitionAwareFunctionWrapper.apply(PartitionAwareFunctionWrapper.java:77) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionWrapper.apply(FunctionConfiguration.java:727) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1.handleMessageInternal(FunctionConfiguration.java:560) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:56) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:115) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:133) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:106) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:72) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:317) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:272) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:187) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:166) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:47) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:109) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:208) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.sendMessageIfAny(KafkaMessageDrivenChannelAdapter.java:398) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:79) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:457) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:431) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.lambda$onMessage$0(RetryingMessageListenerAdapter.java:123) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:329) [spring-retry-1.3.1.jar:?]
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:255) [spring-retry-1.3.1.jar:?]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:117) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:41) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2323) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2304) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2218) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2143) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2025) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1707) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161) [spring-kafka-2.7.6.jar:2.7.6]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_281]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_281]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_281]
14:38:09.889 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.
14:38:09.890 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.metastore.HiveMetaStoreClient ugi=gmjk (auth:SIMPLE) retries=1 delay=1 lifetime=0
14:38:09.891 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Connected to Hive metastore
14:38:32.693 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] ERROR org.springframework.integration.handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1@59cadefd]; nested exception is org.apache.flink.table.catalog.exceptions.CatalogException: Configured default database default doesn't exist in catalog ods_test., failedMessage=GenericMessage [payload=byte[3680], headers={skip-input-type-conversion=false, kafka_offset=8, kafka_consumer=org.apache.kafka.clients.consumer.KafkaConsumer@1a3ce2de, deliveryAttempt=3, kafka_timestampType=CREATE_TIME, kafka_receivedPartitionId=0, kafka_receivedMessageKey=[B@1eb2ef63, kafka_receivedTopic=catalog-meta-data, kafka_receivedTimestamp=1706861090328, contentType=application/json, kafka_groupId=yanshiaaaaa}]
	at org.springframework.integration.support.utils.IntegrationUtils.wrapInHandlingExceptionIfNecessary(IntegrationUtils.java:192)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:65)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:133)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:106)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:72)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:317)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:272)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:187)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:166)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:47)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:109)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:208)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.sendMessageIfAny(KafkaMessageDrivenChannelAdapter.java:398)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:79)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:457)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:431)
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.lambda$onMessage$0(RetryingMessageListenerAdapter.java:123)
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:329)
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:255)
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:117)
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:41)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2323)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2304)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2218)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2143)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2025)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1707)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Configured default database default doesn't exist in catalog ods_test.
	at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:309)
	at org.apache.flink.table.catalog.CatalogManager.lambda$getCatalog$0(CatalogManager.java:405)
	at java.util.Optional.map(Optional.java:215)
	at org.apache.flink.table.catalog.CatalogManager.getCatalog(CatalogManager.java:402)
	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1287)
	at org.apache.flink.table.catalog.CatalogManager.alterTable(CatalogManager.java:1141)
	at com.rock.cdc.kafka.schema.sync.App.handAlterTable(App.java:132)
	at com.rock.cdc.kafka.schema.sync.App.lambda$sinkProcess$0(App.java:81)
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.invokeConsumer(SimpleFunctionRegistry.java:854)
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.doApply(SimpleFunctionRegistry.java:643)
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.apply(SimpleFunctionRegistry.java:489)
	at org.springframework.cloud.stream.function.PartitionAwareFunctionWrapper.apply(PartitionAwareFunctionWrapper.java:77)
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionWrapper.apply(FunctionConfiguration.java:727)
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1.handleMessageInternal(FunctionConfiguration.java:560)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:56)
	... 32 more

14:38:32.718 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Seeking to offset 8 for partition catalog-meta-data-0
14:38:32.720 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] ERROR org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception; nested exception is org.springframework.kafka.listener.ListenerExecutionFailedException: Listener failed; nested exception is org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1@59cadefd]; nested exception is org.apache.flink.table.catalog.exceptions.CatalogException: Configured default database default doesn't exist in catalog ods_test., failedMessage=GenericMessage [payload=byte[3680], headers={skip-input-type-conversion=false, kafka_offset=8, kafka_consumer=org.apache.kafka.clients.consumer.KafkaConsumer@1a3ce2de, deliveryAttempt=3, kafka_timestampType=CREATE_TIME, kafka_receivedPartitionId=0, kafka_receivedMessageKey=[B@1eb2ef63, kafka_receivedTopic=catalog-meta-data, kafka_receivedTimestamp=1706861090328, contentType=application/json, kafka_groupId=yanshiaaaaa}]
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:206) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.SeekToCurrentErrorHandler.handle(SeekToCurrentErrorHandler.java:112) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2360) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2229) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2143) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2025) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1707) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161) [spring-kafka-2.7.6.jar:2.7.6]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_281]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_281]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_281]
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener failed; nested exception is org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1@59cadefd]; nested exception is org.apache.flink.table.catalog.exceptions.CatalogException: Configured default database default doesn't exist in catalog ods_test., failedMessage=GenericMessage [payload=byte[3680], headers={skip-input-type-conversion=false, kafka_offset=8, kafka_consumer=org.apache.kafka.clients.consumer.KafkaConsumer@1a3ce2de, deliveryAttempt=3, kafka_timestampType=CREATE_TIME, kafka_receivedPartitionId=0, kafka_receivedMessageKey=[B@1eb2ef63, kafka_receivedTopic=catalog-meta-data, kafka_receivedTimestamp=1706861090328, contentType=application/json, kafka_groupId=yanshiaaaaa}]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2376) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2343) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2304) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2218) ~[spring-kafka-2.7.6.jar:2.7.6]
	... 9 more
Caused by: org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1@59cadefd]; nested exception is org.apache.flink.table.catalog.exceptions.CatalogException: Configured default database default doesn't exist in catalog ods_test.
	at org.springframework.integration.support.utils.IntegrationUtils.wrapInHandlingExceptionIfNecessary(IntegrationUtils.java:192) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:65) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:115) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:133) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:106) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:72) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:317) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:272) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:187) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:166) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:47) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:109) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:208) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.sendMessageIfAny(KafkaMessageDrivenChannelAdapter.java:398) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:79) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:457) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:431) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.lambda$onMessage$0(RetryingMessageListenerAdapter.java:123) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:329) ~[spring-retry-1.3.1.jar:?]
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:255) ~[spring-retry-1.3.1.jar:?]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:117) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:41) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2323) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2304) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2218) ~[spring-kafka-2.7.6.jar:2.7.6]
	... 9 more
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Configured default database default doesn't exist in catalog ods_test.
	at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:309) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.lambda$getCatalog$0(CatalogManager.java:405) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at java.util.Optional.map(Optional.java:215) ~[?:1.8.0_281]
	at org.apache.flink.table.catalog.CatalogManager.getCatalog(CatalogManager.java:402) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1287) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.alterTable(CatalogManager.java:1141) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at com.rock.cdc.kafka.schema.sync.App.handAlterTable(App.java:132) ~[classes/:?]
	at com.rock.cdc.kafka.schema.sync.App.lambda$sinkProcess$0(App.java:81) ~[classes/:?]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.invokeConsumer(SimpleFunctionRegistry.java:854) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.doApply(SimpleFunctionRegistry.java:643) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.apply(SimpleFunctionRegistry.java:489) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.PartitionAwareFunctionWrapper.apply(PartitionAwareFunctionWrapper.java:77) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionWrapper.apply(FunctionConfiguration.java:727) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1.handleMessageInternal(FunctionConfiguration.java:560) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:56) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:115) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:133) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:106) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:72) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:317) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:272) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:187) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:166) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:47) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:109) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:208) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.sendMessageIfAny(KafkaMessageDrivenChannelAdapter.java:398) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:79) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:457) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:431) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.lambda$onMessage$0(RetryingMessageListenerAdapter.java:123) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:329) ~[spring-retry-1.3.1.jar:?]
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:255) ~[spring-retry-1.3.1.jar:?]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:117) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:41) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2323) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2304) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2218) ~[spring-kafka-2.7.6.jar:2.7.6]
	... 9 more
14:38:32.727 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Giving away all assigned partitions as lost since generation has been reset,indicating that consumer is no longer part of the group
14:38:32.727 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Lost previously assigned partitions catalog-meta-data-6, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-0, catalog-meta-data-1
14:38:32.731 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.springframework.cloud.stream.binder.kafka.KafkaMessageChannelBinder$2 - yanshiaaaaa: partitions lost: [catalog-meta-data-6, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-0, catalog-meta-data-1]
14:38:32.734 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.springframework.cloud.stream.binder.kafka.KafkaMessageChannelBinder$2 - yanshiaaaaa: partitions revoked: [catalog-meta-data-6, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-0, catalog-meta-data-1]
14:38:32.734 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] (Re-)joining group
14:38:32.744 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] (Re-)joining group
14:38:35.751 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Successfully joined group with generation Generation{generationId=3, memberId='consumer-yanshiaaaaa-2-e33c3564-c67a-47c9-81ab-740c0bb61931', protocol='range'}
14:38:35.752 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Finished assignment for group at generation 3: {consumer-yanshiaaaaa-2-e33c3564-c67a-47c9-81ab-740c0bb61931=Assignment(partitions=[catalog-meta-data-0, catalog-meta-data-1, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-6])}
14:38:35.769 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Successfully synced group in generation Generation{generationId=3, memberId='consumer-yanshiaaaaa-2-e33c3564-c67a-47c9-81ab-740c0bb61931', protocol='range'}
14:38:35.770 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Notifying assignor about the new Assignment(partitions=[catalog-meta-data-0, catalog-meta-data-1, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-6])
14:38:35.770 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Adding newly assigned partitions: catalog-meta-data-6, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-0, catalog-meta-data-1
14:38:35.777 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-6
14:38:35.779 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-4
14:38:35.779 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-5
14:38:35.780 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-2
14:38:35.780 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-3
14:38:35.780 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-0
14:38:35.780 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Found no committed offset for partition catalog-meta-data-1
14:38:35.786 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.119:9092 (id: 3 rack: null)], epoch=absent}}.
14:38:35.787 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.119:9092 (id: 3 rack: null)], epoch=absent}}.
14:38:35.788 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.80:9092 (id: 5 rack: null)], epoch=absent}}.
14:38:35.788 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.68:9092 (id: 4 rack: null)], epoch=absent}}.
14:38:35.792 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.117:9092 (id: 2 rack: null)], epoch=absent}}.
14:38:35.792 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.117:9092 (id: 2 rack: null)], epoch=absent}}.
14:38:35.792 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Resetting offset for partition catalog-meta-data-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[10.195.40.142:9092 (id: 1 rack: null)], epoch=absent}}.
14:38:35.792 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.springframework.cloud.stream.binder.kafka.KafkaMessageChannelBinder$2 - yanshiaaaaa: partitions assigned: [catalog-meta-data-6, catalog-meta-data-4, catalog-meta-data-5, catalog-meta-data-2, catalog-meta-data-3, catalog-meta-data-0, catalog-meta-data-1]
14:38:45.649 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Setting hive conf dir as /Users/yanshi
14:38:46.217 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Created HiveCatalog 'ods_test'
14:38:46.274 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Trying to connect to metastore with URI thrift://bigdata-hiveserver-41-16.bigdata.prod:9083
14:38:46.279 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 4
14:38:46.305 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] WARN  org.apache.hadoop.security.ShellBasedUnixGroupsMapping - unable to return groups for user gmjk
org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException: The user name 'gmjk' is not found. id: gmjk: no such user
id: gmjk: no such user

	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:387) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:321) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:270) ~[hadoop-common-3.1.3.jar:?]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3952) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974) ~[guava-27.0-jre.jar:?]
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958) ~[guava-27.0-jre.jar:?]
	at org.apache.hadoop.security.Groups.getGroups(Groups.java:228) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1587) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1575) ~[hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:534) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:224) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_281]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_281]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_281]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_281]
	at org.apache.hadoop.hive.metastore.utils.JavaUtils.newInstance(JavaUtils.java:84) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:95) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:148) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-standalone-metastore-3.1.2.jar:3.1.2]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_281]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_281]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]
	at org.apache.flink.table.catalog.hive.client.HiveShimV310.getHiveMetastoreClient(HiveShimV310.java:140) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:288) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:89) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:79) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:32) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:303) ~[flink-connector-hive_2.12-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.lambda$getCatalog$0(CatalogManager.java:405) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at java.util.Optional.map(Optional.java:215) ~[?:1.8.0_281]
	at org.apache.flink.table.catalog.CatalogManager.getCatalog(CatalogManager.java:402) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1287) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at org.apache.flink.table.catalog.CatalogManager.alterTable(CatalogManager.java:1141) ~[flink-table-api-java-1.18.0.jar:1.18.0]
	at com.rock.cdc.kafka.schema.sync.App.handAlterTable(App.java:132) ~[classes/:?]
	at com.rock.cdc.kafka.schema.sync.App.lambda$sinkProcess$0(App.java:81) ~[classes/:?]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.invokeConsumer(SimpleFunctionRegistry.java:854) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.doApply(SimpleFunctionRegistry.java:643) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.function.context.catalog.SimpleFunctionRegistry$FunctionInvocationWrapper.apply(SimpleFunctionRegistry.java:489) ~[spring-cloud-function-context-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.PartitionAwareFunctionWrapper.apply(PartitionAwareFunctionWrapper.java:77) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionWrapper.apply(FunctionConfiguration.java:727) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1.handleMessageInternal(FunctionConfiguration.java:560) ~[spring-cloud-stream-3.1.3.jar:3.1.3]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:56) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:115) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:133) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:106) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:72) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:317) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:272) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:187) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:166) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:47) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:109) ~[spring-messaging-5.3.9.jar:5.3.9]
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:208) ~[spring-integration-core-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.sendMessageIfAny(KafkaMessageDrivenChannelAdapter.java:398) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:79) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:457) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$IntegrationRecordMessageListener.onMessage(KafkaMessageDrivenChannelAdapter.java:431) ~[spring-integration-kafka-5.5.3.jar:5.5.3]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.lambda$onMessage$0(RetryingMessageListenerAdapter.java:123) ~[spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:329) [spring-retry-1.3.1.jar:?]
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:255) [spring-retry-1.3.1.jar:?]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:117) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.adapter.RetryingMessageListenerAdapter.onMessage(RetryingMessageListenerAdapter.java:41) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2323) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2304) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2218) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2143) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2025) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:1707) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1274) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1266) [spring-kafka-2.7.6.jar:2.7.6]
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1161) [spring-kafka-2.7.6.jar:2.7.6]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_281]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_281]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_281]
14:38:46.350 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.
14:38:46.351 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.metastore.HiveMetaStoreClient ugi=gmjk (auth:SIMPLE) retries=1 delay=1 lifetime=0
14:38:46.353 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Connected to Hive metastore
14:38:55.414 [kafka-coordinator-heartbeat-thread | yanshiaaaaa] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Group coordinator 10.195.40.80:9092 (id: 2147483642 rack: null) is unavailable or invalid due to cause: session timed out without receiving a heartbeat response.isDisconnected: false. Rediscovery will be attempted.
14:38:55.730 [kafka-coordinator-heartbeat-thread | yanshiaaaaa] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Discovered group coordinator 10.195.40.80:9092 (id: 2147483642 rack: null)
14:38:55.731 [kafka-coordinator-heartbeat-thread | yanshiaaaaa] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Discovered group coordinator 10.195.40.80:9092 (id: 2147483642 rack: null)
14:38:55.731 [kafka-coordinator-heartbeat-thread | yanshiaaaaa] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Discovered group coordinator 10.195.40.80:9092 (id: 2147483642 rack: null)
14:39:02.812 [kafka-coordinator-heartbeat-thread | yanshiaaaaa] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator - [Consumer clientId=consumer-yanshiaaaaa-2, groupId=yanshiaaaaa] Attempt to heartbeat with Generation{generationId=3, memberId='consumer-yanshiaaaaa-2-e33c3564-c67a-47c9-81ab-740c0bb61931', protocol='range'} and group instance id Optional.empty failed due to UNKNOWN_MEMBER_ID, resetting generation
14:39:09.521 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Setting hive conf dir as /Users/yanshi
14:39:09.534 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Created HiveCatalog 'ods_test'
14:39:09.536 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Trying to connect to metastore with URI thrift://bigdata-hiveserver-41-16.bigdata.prod:9083
14:39:09.538 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 5
14:39:09.561 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.
14:39:09.561 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.metastore.HiveMetaStoreClient ugi=gmjk (auth:SIMPLE) retries=1 delay=1 lifetime=0
14:39:09.561 [KafkaConsumerDestination{consumerDestinationName='catalog-meta-data', partitions=0, dlqName='null'}.container-0-C-1] INFO  org.apache.flink.table.catalog.hive.HiveCatalog - Connected to Hive metastore
